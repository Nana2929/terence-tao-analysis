\section{The several variable calculus chain rule}\label{sec 6.4}

\begin{theorem}[Several variable calculus chain rule]\label{6.4.1}
    Let \(E\) be a subset of \(\mathbf{R}^n\), and let \(F\) be a subset of \(\mathbf{R}^m\).
    Let \(f : E \to F\) be a function, and let \(g : F \to \mathbf{R}^p\) be another function.
    Let \(x_0\) be a point in the interior of \(E\).
    Suppose that \(f\) is differentiable at \(x_0\), and that \(f(x_0)\) is in the interior of \(F\).
    Suppose also that \(g\) is differentiable at \(f(x_0)\).
    Then \(g \circ f : E \to \mathbf{R}^p\) is also differentiable at \(x_0\), and we have the formula
    \[
        (g \circ f)'(x_0) = g'\big(f(x_0)\big) \circ f'(x_0).
    \]
\end{theorem}

\begin{proof}
    By Definition \ref{6.2.2} we want to show that
    \[
        \lim_{x \to x_0 ; x \in E \setminus \{x_0\}} \frac{\norm*{(g \circ f)(x) - (g \circ f)(x_0) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big)(x - x_0)}}{\norm*{x - x_0}} = 0.
    \]
    Equivalently, we want to show that
    \begin{align*}
                 & \forall\ \varepsilon \in \mathbf{R}^+, \exists\ \delta \in \mathbf{R}^+ : \forall\ x \in E \setminus \{x_0\}, \norm*{x - x_0} < \delta \\
        \implies & \frac{\norm*{(g \circ f)(x) - (g \circ f)(x_0) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big)(x - x_0)}}{\norm*{x - x_0}} < \varepsilon  \\
        \implies & \norm*{(g \circ f)(x) - (g \circ f)(x_0) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big)(x - x_0)} < \varepsilon \norm*{x - x_0}.
    \end{align*}
    Since \(f'(x_0)\) exists, we know that
    \[
        \lim_{x \to x_0 ; x \in E \setminus \{x_0\}} \frac{\norm*{f(x) - f(x_0) - f'(x_0)(x - x_0)}}{\norm*{x - x_0}} = 0.
    \]
    Equivalently, we know that
    \begin{align*}
                 & \forall\ \varepsilon_f \in \mathbf{R}^+, \exists\ \delta \in \mathbf{R}^+ : \forall\ x \in E \setminus \{x_0\}, \norm*{x - x_0} < \delta \\
        \implies & \frac{\norm*{f(x) - f(x_0) - f'(x_0)(x - x_0)}}{\norm*{x - x_0}} < \varepsilon_f                                                         \\
        \implies & \norm*{f(x) - f(x_0) - f'(x_0)(x - x_0)} < \varepsilon_f \norm*{x - x_0}                                                                 \\
        \implies & \norm*{f(x) - f(x_0)} < \varepsilon_f \norm*{x - x_0} + \norm*{f'(x_0)(x - x_0)}.
    \end{align*}
    Since \(g'\big(f(x_0)\big)\) exists, we know that
    \[
        \lim_{y \to f(x_0) ; x \in F \setminus \{f(x_0)\}} \frac{\norm*{g(y) - g\big(f(x_0)\big) - g'\big(f(x_0)\big)\big(y - f(x_0)\big)}}{\norm*{y - f(x_0)}} = 0.
    \]
    Equivalently, we know that
    \begin{align*}
                 & \forall\ \varepsilon \in \mathbf{R}^+, \exists\ \delta_g \in \mathbf{R}^+ : \forall\ y \in F \setminus \{f(x_0)\}, \norm*{y - f(x_0)} < \delta_g \\
        \implies & \frac{\norm*{g(y) - g\big(f(x_0)\big) - g'\big(f(x_0)\big)\big(y - f(x_0)\big)}}{\norm*{y - f(x_0)}} < \varepsilon                               \\
        \implies & \norm*{g(y) - g\big(f(x_0)\big) - g'\big(f(x_0)\big)\big(y - f(x_0)\big)} < \varepsilon \norm*{y - f(x_0)}.
    \end{align*}
    Fix one pair of \(\varepsilon\) and \(\delta_g\).
    Then we have
    \begin{align*}
                 & \exists\ \delta \in \mathbf{R}^+ : \forall\ x \in E \setminus \{x_0\}, \norm*{x - x_0} < \delta                    \\
        \implies & \begin{cases}
            \norm*{f(x) - f(x_0)} < \varepsilon \norm*{x - x_0} + \norm*{f'(x_0)(x - x_0)}; \\
            \norm*{f(x) - f(x_0)} < \delta_g;
        \end{cases}                                                                                          \\
        \implies & \begin{cases}
            \norm*{f(x) - f(x_0)} < \varepsilon \norm*{x - x_0} + \norm*{f'(x_0)(x - x_0)}; \\
            \norm*{g\big(f(x)\big) - g\big(f(x_0)\big) - g'\big(f(x_0)\big) \big(f(x) - f(x_0)\big)} < \varepsilon \norm*{f(x) - f(x_0)};
        \end{cases}                                                                                          \\
        \implies & \norm*{g\big(f(x)\big) - g\big(f(x_0)\big) - g'\big(f(x_0)\big) \big(f(x) - f(x_0)\big)}                           \\
                 & < \varepsilon^2 \norm*{x - x_0} + \varepsilon \norm*{f'(x_0)(x - x_0)}                                             \\
        \implies & \norm*{g\big(f(x)\big) - g\big(f(x_0)\big) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big) (x - x_0)}                 \\
                 & < \varepsilon^2 \norm*{x - x_0} + \varepsilon \norm*{f'(x_0)(x - x_0)}                                             \\
                 & \quad + \norm*{g'\big(f(x_0)\big) \big(f(x) - f(x_0)\big) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big) (x - x_0)}.
    \end{align*}
    Since \(g'\big(f(x_0)\big)\) is a linear transformation, by Definition \ref{6.1.6} we know that
    \begin{align*}
         & \norm*{g'\big(f(x_0)\big) \big(f(x) - f(x_0)\big) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big) (x - x_0)} \\
         & = \norm*{g'\big(f(x_0)\big) \big(f(x) - f(x_0) - f'(x_0) (x - x_0)\big)}.
    \end{align*}
    By Exercise \ref{ex 6.1.4} we know that
    \begin{align*}
        \exists\ M \in \mathbf{R}^+ : & \norm*{g'\big(f(x_0)\big) \big(f(x) - f(x_0) - f'(x_0) (x - x_0)\big)} \\
                                      & \leq M \norm*{f(x) - f(x_0) - f'(x_0) (x - x_0)}                       \\
                                      & \leq M \varepsilon \norm*{x - x_0}.
    \end{align*}
    Fix such \(M\).
    Then we have
    \begin{align*}
                 & \exists\ \delta \in \mathbf{R}^+ : \forall\ x \in E \setminus \{x_0\}, \norm*{x - x_0} < \delta         \\
        \implies & \norm*{g\big(f(x)\big) - g\big(f(x_0)\big) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big) (x - x_0)}      \\
                 & < \varepsilon^2 \norm*{x - x_0} + \varepsilon \norm*{f'(x_0)(x - x_0)} + M \varepsilon \norm*{x - x_0}.
    \end{align*}
    Since \(\varepsilon\) is arbitrary, we conclude that
    \begin{align*}
                 & \forall\ \varepsilon \in \mathbf{R}^+, \exists\ \delta \in \mathbf{R}^+ : \forall\ x \in E \setminus \{x_0\}, \norm*{x - x_0} < \delta \\
        \implies & \norm*{g\big(f(x)\big) - g\big(f(x_0)\big) - \Big(g'\big(f(x_0)\big) \circ f'(x_0)\Big) (x - x_0)} < \varepsilon.
    \end{align*}
\end{proof}

\begin{note}
    As a corollary of the chain rule and Lemma \ref{6.1.16} (and Lemma \ref{6.1.13}), we see that
    \[
        D (g \circ f)(x_0) = D g\big(f(x_0)\big) \cdot D f(x_0);
    \]
    i.e., we can write the chain rule in terms of matrices and matrix multiplication, instead of in terms of linear transformations and composition.
\end{note}

\begin{example}\label{6.4.2}
    Let \(f : \mathbf{R}^n \to \mathbf{R}\) and \(g : \mathbf{R}^n \to \mathbf{R}\) be differentiable functions.
    We form the combined function \(h : \mathbf{R}^n \to \mathbf{R}^2\) by defining \(h(x) \coloneqq \big(f(x), g(x)\big)\).
    Now let \(k : \mathbf{R}^2 \to \mathbf{R}\) be the multiplication function \(k(a, b) \coloneqq ab\).

    We first show that
    \[
        D h(x_0) = \begin{pmatrix}
            \nabla f(x_0) \\
            \nabla g(x_0)
        \end{pmatrix}.
    \]
    Let \(x_0 \in \mathbf{R}^n\).
    Since
    \begin{align*}
         & \norm*{h(x) - h(x_0) - (x - x_0) D h(x_0)^\top}                                                                                  \\
         & = \norm*{\big(f(x), g(x)\big) - \big(f(x_0), g(x_0)\big) - \big((x - x_0) \nabla f(x_0)^\top, (x - x_0) \nabla g(x_0)^\top\big)} \\
         & = \norm*{\big(f(x) - f(x_0) - (x - x_0) \nabla f(x_0)^\top, g(x) - g(x_0) - (x - x_0) \nabla g(x_0)^\top\big)}                   \\
         & \leq \norm*{f(x) - f(x_0) - (x - x_0) \nabla f(x_0)^\top} + \norm*{g(x) - g(x_0) - (x - x_0) \nabla g(x_0)^\top}
    \end{align*}
    (note that the last line follow by Exercise \ref{ex 1.1.8}),
    by squeeze test we know that
    \begin{align*}
                 & \begin{cases}
            \lim_{x \to x_0 ; x \in \mathbf{R}^n \setminus \{x_0\}} \frac{\norm*{f(x) - f(x_0) - f'(x_0)(x - x_0)}}{\norm*{x - x_0}} = 0 \\
            \lim_{x \to x_0 ; x \in \mathbf{R}^n \setminus \{x_0\}} \frac{\norm*{g(x) - g(x_0) - g'(x_0)(x - x_0)}}{\norm*{x - x_0}} = 0
        \end{cases}                                                                                                           \\
        \implies & \begin{cases}
            \lim_{x \to x_0 ; x \in \mathbf{R}^n \setminus \{x_0\}} \frac{\norm*{f(x) - f(x_0) - (x - x_0) \nabla f(x_0)^\top}}{\norm*{x - x_0}} = 0 \\
            \lim_{x \to x_0 ; x \in \mathbf{R}^n \setminus \{x_0\}} \frac{\norm*{g(x) - g(x_0) - (x - x_0) \nabla g(x_0)^\top}}{\norm*{x - x_0}} = 0
        \end{cases}                                                                                                           \\
        \implies & \lim_{x \to x_0 ; x \in \mathbf{R}^n \setminus \{x_0\}} \frac{\norm*{h(x) - h(x_0) - (x - x_0) D h(x_0)^\top}}{\norm*{x - x_0}} = 0.
    \end{align*}
    Since \(x_0\) is arbitrary, we conclude that the identity is true.

    Now we show that
    \[
        D k(a, b) = (b, a).
    \]
    Let \((a, b) \in \mathbf{R}^2\).
    Observe that for any \((x, y) \in \mathbf{R}^2 \setminus \{(a, b)\}\), we have
    \begin{align*}
         & \frac{\norm*{k(x, y) - k(a, b) - \big((x, y) - (a, b)\big) (b, a)^\top}}{\norm*{(x, y) - (a, b)}} \\
         & = \frac{\norm*{xy - ab - xb - ay + ab + ba}}{\norm*{(x - a, y - b)}}                              \\
         & = \frac{\norm*{(x - a)(y - b)}}{\norm*{(x - a, y - b)}}                                           \\
         & = \sqrt{\frac{(x - a)^2 (y - b)^2}{(x - a)^2 + (y - b)^2}}                                        \\
         & \leq \sqrt{\frac{2 (x - a)^2 (y - b)^2}{(x - a)^2 + (y - b)^2}}                                   \\
         & \leq \sqrt{\frac{(x - a)^4 + 2 (x - a)^2 (y - b)^2 + (y - b)^4}{(x - a)^2 + (y - b)^2}}           \\
         & = \sqrt{\frac{\big((x - a)^2 + (y - b)^2\big)^2}{(x - a)^2 + (y - b)^2}}                          \\
         & = \sqrt{(x - a)^2 + (y - b)^2}.
    \end{align*}
    Since
    \begin{align*}
                 & \lim_{(x, y) \to (a, b) ; (x, y) \in \mathbf{R}^2 \setminus \{(a, b)\}} (x - a)^2 + (y - b)^2 = 0                                                                             \\
        \implies & \lim_{(x, y) \to (a, b) ; (x, y) \in \mathbf{R}^2 \setminus \{(a, b)\}} \sqrt{(x - a)^2 + (y - b)^2} = 0                                                                      \\
        \implies & \lim_{(x, y) \to (a, b) ; (x, y) \in \mathbf{R}^2 \setminus \{(a, b)\}} \frac{\norm*{k(x, y) - k(a, b) - \big((x, y) - (a, b)\big) (b, a)^\top}}{\norm*{(x, y) - (a, b)}} = 0
    \end{align*}
    (note that the last line follow by squeeze test),
    by Definition \ref{6.2.2} we know that the identity is true.

    By the chain rule, we thus see that
    \[
        D (k \circ h)(x_0) = \big(g(x_0), f(x_0)\big) \begin{pmatrix}
            \nabla f(x_0) \\
            \nabla g(x_0)
        \end{pmatrix} = g(x_0) \nabla f(x_0) + f(x_0) \nabla g(x_0).
    \]
    But \(k \circ h = fg\), and \(D (fg) = \nabla (fg)\).
    We have thus proven the \emph{product rule}
    \[
        \nabla (fg) = g \nabla f + f \nabla g.
    \]
    A similar argument gives the sum rule \(\nabla (f + g) = \nabla f + \nabla g\), or the difference rule \(\nabla (f - g) = \nabla f - \nabla g\), as well as the quotient rule (Exercise \ref{ex 6.4.4}).
\end{example}

\begin{note}
    We do record one further useful application of the chain rule.
    Let \(T : \mathbf{R}^n \to \mathbf{R}^m\) be a linear transformation.
    From Exercise \ref{ex 6.4.1} we observe that \(T\) is continuously differentiable at every point, and in fact \(T'(x) = T\) for every \(x\).
    (This equation may look a little strange, but perhaps it is easier to swallow if you view it in the form \(\frac{d}{dx} (Tx) = T\).)
    Thus, for any differentiable function \(f : E \to \mathbf{R}^n\), we see that \(T f : E \to \mathbf{R}^m\) is also differentiable, and hence by the chain rule
    \[
        (T f)'(x_0) = T\big(f'(x_0)\big).
    \]
    This is a generalization of the single-variable calculus rule \((cf)' = c(f')\) for constant scalars \(c\).
\end{note}

\begin{note}
    Another special case of the chain rule which is quite useful is the following:
    if \(f : \mathbf{R}^n \to \mathbf{R}^m\) is some differentiable function, and \(x_j : \mathbf{R} \to \mathbf{R}\) are differentiable functions for each \(j = 1, \dots n\), then
    \[
        \frac{d}{dt} f\big(x_1(t), x_2(t), \dots, x_n(t)\big) = \sum_{j = 1}^n x_j'(t) \frac{\partial f}{\partial x_j} \big(x_1(t), x_2(t), \dots, x_n(t)\big).
    \]
\end{note}

\exercisesection

\begin{exercise}\label{ex 6.4.1}
    Let \(T : \mathbf{R} \to \mathbf{R}^m\) be a linear transformation.
    Show that \(T\) is continuously differentiable at every point, and in fact \(T'(x) = T\) for every \(x\).
    What is \(D T\)?
\end{exercise}

\begin{exercise}\label{ex 6.4.2}
    Let \(E\) be a subset of \(\mathbf{R}^n\).
    Prove that if a function \(f : E \to \mathbf{R}^m\) is differentiable at an interior point \(x_0\) of \(E\), then it is also continuous at \(x_0\).
\end{exercise}

\begin{exercise}\label{ex 6.4.3}
    Prove Theorem \ref{6.4.1}.
\end{exercise}

\begin{proof}
    See Theorem \ref{6.4.1}.
\end{proof}

\begin{exercise}\label{ex 6.4.4}
    State and prove some version of the quotient rule for functions of several variables (i.e., functions of the form \(f : E \to \mathbf{R}\) for some subset \(E\) of \(\mathbf{R}^n\)).
    In other words, state a rule which gives a formula for the gradient of \(f / g\);
    compare your answer with Theorem 10.1.13(h) in Analysis I.
    Be sure to make clear what all your assumptions are.
\end{exercise}

\begin{exercise}\label{ex 6.4.5}
    Let \(\vec{x} : \mathbf{R} \to \mathbf{R}^3\) be a differentiable function, and let \(r : \mathbf{R} \to \mathbf{R}\) be the function \(r(t) \coloneqq \norm*{\vec{x}(t)}\), where \(\norm*{x}\) denotes the length of \(\vec{x}\) as measured in the usual \(l^2\) metric.
    Let \(t_0\) be a real number.
    Show that if \(r(t_0) \neq 0\), then \(r\) is differentiable at \(t_0\), and
    \[
        r'(t_0) = \frac{\vec{x}'(t_0) \cdot \vec{x}(t_0)}{r(t_0)}.
    \]
\end{exercise}